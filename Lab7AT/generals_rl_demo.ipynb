{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por Refuerzo para Generals.io\n",
    "\n",
    "En este notebook, implementaremos un agente de aprendizaje por refuerzo utilizando Q-learning para jugar Generals.io. Utilizaremos el entorno de generals-bots y visualizaremos el progreso del entrenamiento.\n",
    "\n",
    "**IMPORTANTE**: Antes de ejecutar este notebook, por favor:\n",
    "1. Ve a Runtime -> Restart runtime\n",
    "2. Espera a que el runtime se reinicie completamente\n",
    "3. Luego ejecuta las celdas en orden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalación de Dependencias\n",
    "\n",
    "Primero, necesitamos instalar las bibliotecas necesarias para nuestro proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clonar el repositorio\n",
    "!git clone https://github.com/strakam/generals-bots.git\n",
    "\n",
    "# Instalar dependencias básicas\n",
    "!pip install -q matplotlib pandas\n",
    "!pip install -q pettingzoo\n",
    "\n",
    "# Instalar generals-bots\n",
    "%cd generals-bots\n",
    "!pip install -q -e .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importación de Bibliotecas\n",
    "\n",
    "Importaremos todas las bibliotecas necesarias para nuestro proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generals.envs import PettingZooGenerals\n",
    "from generals import GridFactory\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Verificar la instalación\n",
    "print(\"Versiones de las bibliotecas principales:\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuración del Entorno\n",
    "\n",
    "Configuraremos el entorno de Generals.io con un grid personalizado y las propiedades deseadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del grid\n",
    "grid_factory = GridFactory(\n",
    "    mode=\"generalsio\",  # Usamos el modo que replica el juego oficial\n",
    "    min_grid_dims=(10, 10),\n",
    "    max_grid_dims=(15, 15),\n",
    "    mountain_density=0.2,\n",
    "    city_density=0.05\n",
    ")\n",
    "\n",
    "# Crear el entorno\n",
    "env = PettingZooGenerals(\n",
    "    agents=[\"player1\", \"player2\"],  # Lista de IDs de agentes\n",
    "    grid_factory=grid_factory,\n",
    "    truncation=1000,  # Número máximo de pasos por episodio\n",
    "    speed_multiplier=1.0  # Velocidad del juego\n",
    ")\n",
    "\n",
    "# Verificar el espacio de observación y acción\n",
    "print(\"Espacio de observación:\", env.observation_space)\n",
    "print(\"Espacio de acción:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementación del Agente Q-Learning\n",
    "\n",
    "Implementaremos una clase para nuestro agente Q-learning que aprenderá a jugar Generals.io."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, veamos la estructura de una observación y el espacio de acción\n",
    "observations, infos = env.reset()\n",
    "print(\"Estructura de la observación:\")\n",
    "print(observations[\"player1\"].keys())\n",
    "print(\"\\nDimensiones del grid:\")\n",
    "print(f\"Filas: {observations['player1']['armies'].shape[0]}\")\n",
    "print(f\"Columnas: {observations['player1']['armies'].shape[1]}\")\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.q_table = {}\n",
    "        # Obtener dimensiones del grid de la primera observación\n",
    "        obs, _ = env.reset()\n",
    "        self.grid_rows = obs[\"player1\"][\"armies\"].shape[0]\n",
    "        self.grid_cols = obs[\"player1\"][\"armies\"].shape[1]\n",
    "        # El número de acciones es el número de celdas en el grid\n",
    "        self.num_actions = self.grid_rows * self.grid_cols\n",
    "        print(f\"Número de acciones posibles: {self.num_actions}\")\n",
    "        \n",
    "    def get_state_key(self, obs):\n",
    "        # Convertimos la observación en una clave única\n",
    "        state_key = f\"{obs['armies'].tobytes()}_{obs['generals'].tobytes()}_{obs['cities'].tobytes()}_{obs['mountains'].tobytes()}_{obs['owned_cells'].tobytes()}_{obs['opponent_cells'].tobytes()}\"\n",
    "        return state_key\n",
    "    \n",
    "    def is_valid_position(self, row, col):\n",
    "        return 0 <= row < self.grid_rows and 0 <= col < self.grid_cols\n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        state_key = self.get_state_key(obs)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # Exploración: acción aleatoria válida\n",
    "            # En generals-bots, todas las acciones son válidas si hay unidades en la celda\n",
    "            valid_actions = np.zeros(self.num_actions)  # Inicializamos todas las acciones como inválidas\n",
    "            \n",
    "            # Marcamos como válidas solo las acciones que tienen unidades\n",
    "            for row in range(self.grid_rows):\n",
    "                for col in range(self.grid_cols):\n",
    "                    if obs['armies'][row, col] > 0:\n",
    "                        action_idx = row * self.grid_cols + col\n",
    "                        valid_actions[action_idx] = 1\n",
    "            \n",
    "            valid_indices = np.where(valid_actions == 1)[0]\n",
    "            if len(valid_indices) > 0:\n",
    "                return np.random.choice(valid_indices)\n",
    "            return 0  # Acción por defecto si no hay acciones válidas\n",
    "        \n",
    "        # Explotación: mejor acción según Q-table\n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = np.zeros(self.num_actions)\n",
    "            \n",
    "        q_values = self.q_table[state_key].copy()\n",
    "        # Invalidamos acciones que no tienen unidades\n",
    "        for row in range(self.grid_rows):\n",
    "            for col in range(self.grid_cols):\n",
    "                action_idx = row * self.grid_cols + col\n",
    "                if obs['armies'][row, col] == 0:\n",
    "                    q_values[action_idx] = float('-inf')\n",
    "        \n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "    def update(self, obs, action, reward, next_obs, done):\n",
    "        state_key = self.get_state_key(obs)\n",
    "        next_state_key = self.get_state_key(next_obs)\n",
    "        \n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = np.zeros(self.num_actions)\n",
    "        if next_state_key not in self.q_table:\n",
    "            self.q_table[next_state_key] = np.zeros(self.num_actions)\n",
    "            \n",
    "        current_q = self.q_table[state_key][action]\n",
    "        next_max_q = np.max(self.q_table[next_state_key])\n",
    "        \n",
    "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * next_max_q * (1 - done) - current_q)\n",
    "        self.q_table[state_key][action] = new_q\n",
    "        \n",
    "        if done:\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento del Agente\n",
    "\n",
    "Implementaremos la función de entrenamiento para nuestro agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(episodes=1000, max_steps=1000):\n",
    "    agent = QLearningAgent()\n",
    "    rewards_history = []\n",
    "    wins_history = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        observations, infos = env.reset()\n",
    "        total_reward = 0\n",
    "        won = False\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Turno del agente\n",
    "            action = agent.get_action(observations[\"player1\"])\n",
    "            next_observations, rewards, terminations, truncations, infos = env.step({\"player1\": action, \"player2\": 0})\n",
    "            \n",
    "            agent.update(observations[\"player1\"], action, rewards[\"player1\"], next_observations[\"player1\"], terminations[\"player1\"])\n",
    "            total_reward += rewards[\"player1\"]\n",
    "            \n",
    "            if infos[\"player1\"].get(\"won\", False):\n",
    "                won = True\n",
    "                \n",
    "            if terminations[\"player1\"] or truncations[\"player1\"]:\n",
    "                break\n",
    "                \n",
    "            observations = next_observations\n",
    "            \n",
    "        rewards_history.append(total_reward)\n",
    "        wins_history.append(won)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episodio: {episode}, Recompensa Total: {total_reward:.2f}, Épsilon: {agent.epsilon:.2f}\")\n",
    "            \n",
    "    return rewards_history, wins_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualización de Resultados\n",
    "\n",
    "Implementaremos funciones para visualizar el progreso del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(rewards_history, wins_history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Gráfico de recompensas\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rewards_history)\n",
    "    plt.title('Historial de Recompensas')\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('Recompensa Total')\n",
    "    \n",
    "    # Gráfico de victorias\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(pd.Series(wins_history).rolling(10).mean())\n",
    "    plt.title('Tasa de Victoria (Media Móvil)')\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('Tasa de Victoria')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ejecución del Entrenamiento\n",
    "\n",
    "Ahora ejecutaremos el entrenamiento y visualizaremos los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar el entrenamiento\n",
    "rewards_history, wins_history = train_agent(episodes=1000)\n",
    "\n",
    "# Visualizar resultados\n",
    "plot_training_results(rewards_history, wins_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluación del Agente Entrenado\n",
    "\n",
    "Finalmente, evaluaremos el rendimiento de nuestro agente entrenado en algunos juegos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(episodes=10):\n",
    "    agent = QLearningAgent()\n",
    "    agent.epsilon = 0  # Desactivamos la exploración para la evaluación\n",
    "    \n",
    "    wins = 0\n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        observations, infos = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.get_action(observations[\"player1\"])\n",
    "            next_observations, rewards, terminations, truncations, infos = env.step({\"player1\": action, \"player2\": 0})\n",
    "            \n",
    "            total_reward += rewards[\"player1\"]\n",
    "            observations = next_observations\n",
    "            done = terminations[\"player1\"] or truncations[\"player1\"]\n",
    "            \n",
    "            if infos[\"player1\"].get(\"won\", False):\n",
    "                wins += 1\n",
    "                \n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episodio de evaluación {episode + 1}: Recompensa = {total_reward:.2f}\")\n",
    "        \n",
    "    print(f\"\\nTasa de victoria: {wins/episodes*100:.2f}%\")\n",
    "    print(f\"Recompensa promedio: {np.mean(total_rewards):.2f}\")\n",
    "    \n",
    "    return wins/episodes, np.mean(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el agente\n",
    "win_rate, avg_reward = evaluate_agent(episodes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
