{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento y Evaluación del Agente KUKA Arm\n",
    "\n",
    "Este notebook implementa el entrenamiento y evaluación de un agente de aprendizaje por refuerzo para controlar un brazo robótico KUKA en un entorno simulado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuración del Entorno de Google Colab\n",
    "\n",
    "Primero, montamos Google Drive y configuramos las dependencias necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montar Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Instalar dependencias necesarias\n",
    "!pip install gymnasium pybullet stable-baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración del Entorno\n",
    "\n",
    "Importamos las dependencias necesarias y definimos el entorno del brazo robótico KUKA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from stable_baselines3 import PPO\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "# Configurar rutas de Google Drive\n",
    "DRIVE_PATH = '/content/drive/MyDrive/KUKA_Project'\n",
    "ASSETS_PATH = os.path.join(DRIVE_PATH, 'assets')\n",
    "LOGS_PATH = os.path.join(DRIVE_PATH, 'logs')\n",
    "\n",
    "# Crear directorios necesarios si no existen\n",
    "os.makedirs(ASSETS_PATH, exist_ok=True)\n",
    "os.makedirs(LOGS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subir el archivo glass.urdf a Google Drive\n",
    "\n",
    "Necesitamos subir el archivo glass.urdf a la carpeta assets en Google Drive. Puedes hacerlo manualmente o usar el siguiente bloque para subirlo desde local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Descomenta y ejecuta esta celda para subir el archivo glass.urdf\n",
    "# uploaded = files.upload()\n",
    "# for filename in uploaded.keys():\n",
    "#     if filename == 'glass.urdf':\n",
    "#         !mv {filename} {ASSETS_PATH}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del Entorno KUKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KukaArmEnv(gym.Env):\n",
    "    def __init__(self, render=False):\n",
    "        super().__init__()\n",
    "        self.render_mode = render\n",
    "        self.max_steps = 200\n",
    "        self.step_count = 0\n",
    "        self.episode_count = 0\n",
    "        self.contact_made = False\n",
    "        self.prev_dist = None\n",
    "        self.cup_initial_pos = None\n",
    "\n",
    "        if self.render_mode:\n",
    "            p.connect(p.GUI)\n",
    "        else:\n",
    "            p.connect(p.DIRECT)\n",
    "\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        self.kuka = None\n",
    "        self.cup_id = None\n",
    "\n",
    "        self.action_space = spaces.Box(low=-2.0, high=2.0, shape=(7,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-10.0, high=10.0, shape=(10,), dtype=np.float32)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        p.resetSimulation()\n",
    "        p.setGravity(0, 0, -9.8)\n",
    "        p.loadURDF(\"plane.urdf\")\n",
    "\n",
    "        self.kuka = p.loadURDF(\"kuka_iiwa/model.urdf\", useFixedBase=True)\n",
    "\n",
    "        if self.episode_count < 100:\n",
    "            cup_x = 0.6\n",
    "        else:\n",
    "            cup_x = np.random.uniform(0.5, 0.65)\n",
    "\n",
    "        # Usar la ruta de Google Drive para el archivo glass.urdf\n",
    "        cup_path = os.path.join(ASSETS_PATH, \"glass.urdf\")\n",
    "        self.cup_initial_pos = [cup_x, 0, 0.06]\n",
    "        self.cup_id = p.loadURDF(cup_path, basePosition=self.cup_initial_pos)\n",
    "\n",
    "        initial_angles = [0.0, -0.9, 0.0, -1.4, 0.0, 1.4, 0.0]\n",
    "        for j in range(7):\n",
    "            p.resetJointState(self.kuka, j, targetValue=initial_angles[j])\n",
    "\n",
    "        self.step_count = 0\n",
    "        self.contact_made = False\n",
    "        self.episode_count += 1\n",
    "        self.prev_dist = None\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        joint_states = [p.getJointState(self.kuka, i)[0] for i in range(7)]\n",
    "        cup_pos = p.getBasePositionAndOrientation(self.cup_id)[0]\n",
    "        return np.array(joint_states + list(cup_pos), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        for i in range(7):\n",
    "            p.setJointMotorControl2(self.kuka, i, p.POSITION_CONTROL,\n",
    "                                    targetPosition=action[i], force=200)\n",
    "\n",
    "        for _ in range(10):\n",
    "            p.stepSimulation()\n",
    "            if self.render_mode:\n",
    "                time.sleep(1./240.)\n",
    "\n",
    "        efector_pos = p.getLinkState(self.kuka, 6)[0]\n",
    "        cup_pos, _ = p.getBasePositionAndOrientation(self.cup_id)\n",
    "        dist = np.linalg.norm(np.array(efector_pos) - np.array(cup_pos))\n",
    "        efector_height = efector_pos[2]\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = -0.2 * dist\n",
    "\n",
    "        if self.prev_dist is not None:\n",
    "            if dist < self.prev_dist:\n",
    "                reward += 0.05\n",
    "            else:\n",
    "                reward -= 0.05\n",
    "        self.prev_dist = dist\n",
    "\n",
    "        if efector_height > 0.4:\n",
    "            reward -= 0.1 * (efector_height - 0.4)\n",
    "\n",
    "        y_error = abs(efector_pos[1] - cup_pos[1])\n",
    "        if y_error < 0.05:\n",
    "            reward += 0.05\n",
    "\n",
    "        codo_angle = p.getJointState(self.kuka, 3)[0]\n",
    "        if abs(codo_angle) < 0.5:\n",
    "            reward -= 0.1\n",
    "        elif -1.5 < codo_angle < -0.8:\n",
    "            reward += 0.05\n",
    "\n",
    "        if dist < 0.08:\n",
    "            p.resetBaseVelocity(self.cup_id, linearVelocity=[0, 0, -1.0])\n",
    "            reward = 1.0\n",
    "            terminated = True\n",
    "            return self._get_obs(), reward, terminated, truncated, {}\n",
    "\n",
    "        dx = abs(cup_pos[0] - self.cup_initial_pos[0])\n",
    "        dz = abs(cup_pos[2] - self.cup_initial_pos[2])\n",
    "        if dx > 0.02 or dz > 0.02:\n",
    "            reward = 1.0\n",
    "            terminated = True\n",
    "            return self._get_obs(), reward, terminated, truncated, {}\n",
    "\n",
    "        self.step_count += 1\n",
    "        if self.step_count >= self.max_steps:\n",
    "            reward = -1.0\n",
    "            truncated = True\n",
    "\n",
    "        return self._get_obs(), reward, terminated, truncated, {}\n",
    "\n",
    "    def close(self):\n",
    "        p.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entrenamiento del Agente\n",
    "\n",
    "Ahora entrenaremos el agente usando el algoritmo PPO (Proximal Policy Optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el entorno\n",
    "env = KukaArmEnv(render=False)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=30000)\n",
    "\n",
    "# Guardar el modelo entrenado en Google Drive\n",
    "model_path = os.path.join(DRIVE_PATH, \"ppo_kuka_arm\")\n",
    "model.save(model_path)\n",
    "\n",
    "# Inicializar array para guardar recompensas\n",
    "np.save(os.path.join(LOGS_PATH, \"rewards_kuka.npy\"), [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluación del Agente\n",
    "\n",
    "Evaluaremos el agente entrenado en 50 episodios y analizaremos su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar entorno con renderizado\n",
    "env = KukaArmEnv(render=True)\n",
    "model = PPO.load(model_path, env=env)\n",
    "\n",
    "total_episodes = 50\n",
    "success_count = 0\n",
    "results = []\n",
    "\n",
    "for ep in range(total_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    hit = False\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if reward == 1.0:\n",
    "            hit = True\n",
    "        done = terminated or truncated\n",
    "        step += 1\n",
    "\n",
    "    success = 1 if hit else 0\n",
    "    success_count += success\n",
    "    print(f\"[EPISODIO {ep+1:02d}] Recompensa total: {total_reward:.2f} → {'✅ ÉXITO' if success else '❌ FALLA'}\")\n",
    "    results.append([ep+1, float(total_reward), \"Éxito\" if success else \"Falla\"])\n",
    "\n",
    "# Guardar resultados en CSV en Google Drive\n",
    "csv_path = os.path.join(LOGS_PATH, \"eval_results.csv\")\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Episodio\", \"Recompensa\", \"Resultado\"])\n",
    "    writer.writerows(results)\n",
    "\n",
    "# Imprimir resumen\n",
    "print(\"\\n📊 EVALUACIÓN COMPLETA\")\n",
    "print(f\"Total de episodios: {total_episodes}\")\n",
    "print(f\"Episodios exitosos: {success_count}\")\n",
    "print(f\"Tasa de éxito: {100 * success_count / total_episodes:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualización de Resultados\n",
    "\n",
    "Visualizaremos los resultados del entrenamiento y la evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica de recompensas\n",
    "rewards = [row[1] for row in results]\n",
    "rolling_avg = np.convolve(rewards, np.ones(5)/5, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(rewards, label=\"Recompensa por episodio\")\n",
    "plt.plot(rolling_avg, label=\"Promedio móvil (5)\", linestyle='--')\n",
    "plt.xlabel(\"Episodio\")\n",
    "plt.ylabel(\"Recompensa\")\n",
    "plt.title(\"Evaluación del agente PPO\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar la gráfica en Google Drive\n",
    "plot_path = os.path.join(LOGS_PATH, \"eval_rewards_plot.png\")\n",
    "plt.savefig(plot_path)\n",
    "plt.show()\n",
    "\n",
    "# Cerrar entorno\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
